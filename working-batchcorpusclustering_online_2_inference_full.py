# -*- coding: utf-8 -*-
"""BatchCorpusClustering-Online-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WdAIH5hMcNN-vvLqGO9W_2txQL6C1v1V
"""

#!pip install sentence-transformers

from collections import OrderedDict
import glob
from itertools import chain
import json
import matplotlib.pyplot as plt
import numpy as np
from operator import itemgetter
import os
import pickle
import requests
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.cluster import MiniBatchKMeans
from sklearn.decomposition import IncrementalPCA
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import normalize
import sys
import time
import torch
import yaml

#from google.colab import drive
#from google.colab import files
from sentence_transformers import SentenceTransformer

sns.set_theme()

#drive.mount('/content/drive', force_remount=True)

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/MyDrive/Colab Notebooks/PhD/CorpusClustering/DataSet/"
#!ls -l

class SentenceTransformersEmbedder:
    def __init__(self, model_name, device_type='cuda'):
        self.device = 'cuda' if torch.cuda.is_available() and device_type == 'cuda' else 'cpu'
        self.model = SentenceTransformer(model_name, device=self.device)

    def batch_encode(self, sentence_list):
        return self.model.encode(sentence_list, convert_to_tensor=True)

USE_DOWNSAMPLED_EMBEDDINGS = False #@param {"type": "boolean"}
#DATASET_DIR = './DataSetColab/'
#DATASET_DIR = './DataSet-Lite/'
#DATASET_DIR = './DataSet-Full-Fixed-3/'
DATASET_DIR = './DataSet-Full-Unique/'
FITTED_MBKS_DIR = os.path.join(DATASET_DIR, 'fitted-mbks/')

CLUSTER_BEAM_SIZE = 10 #@param [1, 2, 5, 10, 100] {"type":"raw"}
SENTENCE_BEAM_SIZE = 10 #@param [1, 2, 5, 10, 100] {"type": "raw"}

NUM_CLUSTERS = 1024 #@param [1, 100, 200, 500, 1000, 1024] {"type": "raw"}
KMEANS_MAX_ITER = 3000000 #@param [100000, 500000, 1000000, 3000000, 5000000] {"type": "raw"}
KMEANS_BATCH_SIZE = 2048 #@param [1024, 2048, 4096] {"type": "raw"}

EMBEDDER_BATCH_SIZE = 32768 #@param [32, 64, 128, 256, 512, 1024, 2048, 4096, 16384, 32768] {"type": "raw"}
BATCH_REPORT_FREQ_FACTOR = 10 #@param [1, 10, 100, 1000] {"type": "raw"}

MAX_BIN_SIZE = 2000 #@param[100, 500, 1000, 2000, 5000] {"type": "raw"}
MAX_KMEANS_FITTING_SIZE = 100000 #@param [10000, 50000, 100000, 200000] {"type": "raw"}
MAX_TMP_EMBEDDING_FILES_TO_STORE = 10 #@param [2, 5, 10, 20] {"type": "raw"}
TIME_TO_SLEEP_AFTER_MBK_DOWNLOAD = 180 #@param [60, 120, 180] {"type": "raw"}
TIME_TO_SLEEP_AFTER_BIN_DOWNLOAD = 5 #@param [2, 5, 10, 20] {"type": "raw"}

REQUEST_URL = 'http://localhost:5005/model/parse'
NLU_YAML_PATH = 'C:\\Users\\gabriel.bercanu\\Desktop\\Work\\transcript-creator\\chatbot-github\\Streamlist-chatbot-en\\data\\nlu2.yml'
INTENT_CONFIDENCE_THRESHOLD = 0.9

EMBEDDER = SentenceTransformersEmbedder('all-mpnet-base-v2', device_type='cuda')

def create_tmp_embeddings(embedder):
    emb_to_idx = dict()
    embedded_sentences = []
    downsampled_embedded_sentences = []

    REPORT_RATIO_SCENARIO = .01
    REPORT_RATIO_CONV = 1.1

    example_idx = 0 # acts as a primary key which links the sentence embedding vector with the natural language sentence
    record_idx = 0 # index of the current record file in use
    total_size = 0 # keeps track of the total size of the stored embedding vectors
    EMBEDDING_RECORD_THRESHOLD = (100 * (1 << 20)) # maximum embedding record file size
    DOWNSAMPLED_EMBEDDING_RECORD_THRESHOLD = (100 * (1 << 20)) # 100 mb
    crt_embedding_size = 0
    crt_downsampled_embedding_size = 0

    N_COMPONENTS_IPCA = 64
    BATCH_SIZE_IPCA = 500
    NUM_EXAMPLES_IPCA = 100000
    transformer = IncrementalPCA(n_components=N_COMPONENTS_IPCA, batch_size=BATCH_SIZE_IPCA)
    fitted_examples = 0
    finished_training = False

    TEXT_RECORDS_DIR = os.path.join(DATASET_DIR, 'text-records/')
    DOWNSAMPLED_EMBEDDINGS_RECORDS_DIR = os.path.join(DATASET_DIR, 'downsampled-embedding-records/')
    NORMALIZED_DOWNSAMPLED_EMBEDDINGS_RECORDS_DIR = os.path.join(DATASET_DIR, 'normalized-downsampled-embedding-records/')
    TEMPORARY_EMBEDDINGS_DIR = os.path.join(DATASET_DIR, 'tmp-embeddings/')
    UNCOMPRESSED_BINS_DIR = os.path.join(DATASET_DIR, 'bins-uncompressed/')
    FITTED_MBKS_DIR = os.path.join(DATASET_DIR, 'fitted-mbks/')

    kmeans = MiniBatchKMeans(n_clusters=NUM_CLUSTERS, random_state=0, max_iter=KMEANS_MAX_ITER, batch_size=KMEANS_BATCH_SIZE)
    crt_kmeans_dump_idx = 0

    X = None
    initialized_x = False

    bins = dict()
    bins_idx = dict()

    all_records = os.listdir(TEXT_RECORDS_DIR)
    all_records.sort()
    crt_tmp_emb_idx = 0
    processed_examples = 0
    
    num_loaded_examples = 0
    kmeans_x_examples = 0
    num_embedded_examples = 0
    num_dumped_examples = 0

    for idx_record, record in enumerate(all_records):
        print('Processing record {}/{} ({})'.format(idx_record, len(all_records), record))
        f = open(os.path.join(TEXT_RECORDS_DIR, record), 'rb')
        obj = pickle.load(f)

        flattened_data = list(chain.from_iterable(obj))
        num_loaded_examples += len(flattened_data)
        
        #599+285+98=982

        for i in range(0, len(flattened_data), EMBEDDER_BATCH_SIZE):
            if i % (EMBEDDER_BATCH_SIZE * BATCH_REPORT_FREQ_FACTOR) == 0:
                print('\tProcessing batch {}/{}'.format(i, len(flattened_data)))
            crt_data_batch = flattened_data[i : i+EMBEDDER_BATCH_SIZE]
            start = time.time()
            crt_embedding = embedder.batch_encode(crt_data_batch)
            num_embedded_examples += len(crt_data_batch)
            end = time.time()
            
            print('Encoded {} examples in {} seconds'.format(EMBEDDER_BATCH_SIZE, (end - start)))

            start = time.time()
            if not initialized_x:
                X = crt_embedding
                initialized_x = True
            else:
                #X = np.vstack((X, crt_embedding))
                X = torch.vstack((X, crt_embedding))
            end = time.time()

            #print('X shape: {}'.format(X.shape))

            # if X gets too large, dump its embedding contents to temporary embedding files
            if X.shape[0] > MAX_KMEANS_FITTING_SIZE:# 100000:
                print('\t\tDumping crt embeddings vector at i = {} (X size was {})'.format(i, X.shape[0]))
                print('\t\tLast stacking took {} seconds'.format(end - start))
                initialized_x = False
                f = open(os.path.join(TEMPORARY_EMBEDDINGS_DIR, 'emb-{:04d}.bin'.format(crt_tmp_emb_idx)), 'wb')
                #pickle.dump(X, f)
                pickle.dump(X.detach().cpu().numpy(), f)
                crt_tmp_emb_idx += 1
                f.close()

            # if after dumping crt file, the number of files gets too big (`MAX_TMP_EMBEDDING_FILES_TO_STORE` files), process their contents and remove them
            if crt_tmp_emb_idx > 0 and crt_tmp_emb_idx % MAX_TMP_EMBEDDING_FILES_TO_STORE == 0: #  10 == 0:
                # fit kmeans on 10 temporary embedding files at once, then remove them
                #crt_tmp_emb_idx = 0
                tmp_embds_files = os.listdir(TEMPORARY_EMBEDDINGS_DIR)
                tmp_embds_files.sort()
                kmeans_X = None
                initialized_kmeans_X = False
                for idx_tmp_emb, tmp_emb in enumerate(tmp_embds_files):
                    full_tmp_emb_path = os.path.join(TEMPORARY_EMBEDDINGS_DIR, tmp_emb)
                    print('Trying to load embedding file {}'.format(full_tmp_emb_path))
                    crt_tmp_emb_f = open(full_tmp_emb_path, 'rb')
                    crt_tmp_emb_data = pickle.load(crt_tmp_emb_f)
                    if not initialized_kmeans_X:
                        initialized_kmeans_X = True
                        kmeans_X = crt_tmp_emb_data
                    else:
                        kmeans_X = np.vstack((kmeans_X, crt_tmp_emb_data))
                    crt_tmp_emb_f.close()
                    
                # partial fit kmeans obj
                print('\t\tStarted Kmeans partial fitting with {} examples'.format(kmeans_X.shape[0]))
                start_kmeans = time.time()
                kmeans = kmeans.partial_fit(kmeans_X)
                kmeans_x_examples += kmeans_X.shape[0]
                end_kmeans = time.time()
                print('\t\tEnded Kmeans partial fitting in {} seconds'.format(end_kmeans - start_kmeans))

                # dump partially fit kmeans obj
                crt_kmeans_obj_path = os.path.join(FITTED_MBKS_DIR, 'fitted-mbk-{:04d}.bin'.format(crt_kmeans_dump_idx))
                g = open(crt_kmeans_obj_path, 'wb')
                pickle.dump(kmeans, g)
                g.close()
                crt_kmeans_dump_idx += 1

                # add newly clustered examples to the bins
                for idx_label, label in enumerate(kmeans.labels_):
                    if not label in bins:
                        bins[label] = [(kmeans_X[idx_label], processed_examples + idx_label)]
                    else:
                        # dump large bins into partial bin file
                        bins[label].append((kmeans_X[idx_label], processed_examples + idx_label))
                        if len(bins[label]) > MAX_BIN_SIZE:
                            print('\t\t\tOffloading cluster {} ({})'.format(label, bins_idx.get(label, 0)))
                            crt_bin_file_idx = bins_idx.get(label, 0)
                            crt_bin_file_path = os.path.join(UNCOMPRESSED_BINS_DIR, 'bin-{:04d}-{:04d}.bin'.format(label, crt_bin_file_idx))
                            f = open(crt_bin_file_path, 'wb')
                            pickle.dump(bins[label], f)
                            num_dumped_examples += len(bins[label])
                            #print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Dumped another {} examples (total: {})'.format(len(bins[label]), num_dumped_examples))
                            f.close()
                            bins_idx[label] = crt_bin_file_idx + 1
                            del bins[label]

                processed_examples += kmeans_X.shape[0]

                # remove tmp embds files after processing them
                for idx_tmp_emb, tmp_emb in enumerate(tmp_embds_files):
                    full_tmp_emb_path = os.path.join(TEMPORARY_EMBEDDINGS_DIR, tmp_emb)
                    os.remove(full_tmp_emb_path)
                crt_tmp_emb_idx = 0
                
                
                
                
                






        # check if there is any remaining temporary embeddings files (less than `MAX_TMP_EMBEDDING_FILES_TO_STORE`)
        tmp_embds_files = os.listdir(TEMPORARY_EMBEDDINGS_DIR)
        if len(tmp_embds_files) > 0:
            tmp_embds_files.sort()
            kmeans_X = None
            initialized_kmeans_X = False
            for idx_tmp_emb, tmp_emb in enumerate(tmp_embds_files):
                full_tmp_emb_path = os.path.join(TEMPORARY_EMBEDDINGS_DIR, tmp_emb)
                print('Trying to load REMAINING embedding file {}'.format(full_tmp_emb_path))
                crt_tmp_emb_f = open(full_tmp_emb_path, 'rb')
                crt_tmp_emb_data = pickle.load(crt_tmp_emb_f)
                if not initialized_kmeans_X:
                    initialized_kmeans_X = True
                    kmeans_X = crt_tmp_emb_data
                else:
                    kmeans_X = np.vstack((kmeans_X, crt_tmp_emb_data))
                crt_tmp_emb_f.close()
            
            # partial fit kmeans obj
            print('\t\tStarted Kmeans partial fitting with {} examples'.format(kmeans_X.shape[0]))
            start_kmeans = time.time()
            kmeans = kmeans.partial_fit(kmeans_X)
            kmeans_x_examples += kmeans_X.shape[0]
            end_kmeans = time.time()
            print('\t\tEnded Kmeans partial fitting in {} seconds'.format(end_kmeans - start_kmeans))

            # dump partially fit kmeans obj
            crt_kmeans_obj_path = os.path.join(FITTED_MBKS_DIR, 'fitted-mbk-{:04d}.bin'.format(crt_kmeans_dump_idx))
            g = open(crt_kmeans_obj_path, 'wb')
            pickle.dump(kmeans, g)
            g.close()
            crt_kmeans_dump_idx += 1

            # add newly clustered examples to the bins
            for idx_label, label in enumerate(kmeans.labels_):
                if not label in bins:
                    bins[label] = [(kmeans_X[idx_label], processed_examples + idx_label)]
                else:
                    # dump large bins into partial bin file
                    bins[label].append((kmeans_X[idx_label], processed_examples + idx_label))
                    if len(bins[label]) > MAX_BIN_SIZE:
                        print('\t\t\tOffloading cluster {} ({})'.format(label, bins_idx.get(label, 0)))
                        crt_bin_file_idx = bins_idx.get(label, 0)
                        crt_bin_file_path = os.path.join(UNCOMPRESSED_BINS_DIR, 'bin-{:04d}-{:04d}.bin'.format(label, crt_bin_file_idx))
                        f = open(crt_bin_file_path, 'wb')
                        pickle.dump(bins[label], f)
                        num_dumped_examples += len(bins[label])
                        #print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Dumped another {} examples (total: {})'.format(len(bins[label]), num_dumped_examples))
                        f.close()
                        bins_idx[label] = crt_bin_file_idx + 1
                        del bins[label]

            print('Adding nothing (kmeans_X shape is {})'.format(kmeans_X.shape))
            processed_examples += kmeans_X.shape[0]

            # remove tmp embds files after processing them
            for idx_tmp_emb, tmp_emb in enumerate(tmp_embds_files):
                full_tmp_emb_path = os.path.join(TEMPORARY_EMBEDDINGS_DIR, tmp_emb)
                os.remove(full_tmp_emb_path)
            crt_tmp_emb_idx = 0


















        



        # fit kmeans with remaining X data
        print('\t\tStarted Kmeans partial fitting with remaining X examples ({})'.format(X.shape[0]))
        start_kmeans = time.time()
        kmeans_X = X.detach().cpu().numpy()
        kmeans = kmeans.partial_fit(kmeans_X)
        kmeans_x_examples += kmeans_X.shape[0]
        end_kmeans = time.time()
        print('\t\tEnded Kmeans partial fitting in {} seconds'.format(end_kmeans - start_kmeans))

        # dump partially fit kmeans obj (with remaining X data)
        crt_kmeans_obj_path = os.path.join(FITTED_MBKS_DIR, 'fitted-mbk-{:04d}.bin'.format(crt_kmeans_dump_idx))
        g = open(crt_kmeans_obj_path, 'wb')
        pickle.dump(kmeans, g)
        g.close()
        crt_kmeans_dump_idx += 1

        # add newly clustered examples to the bins
        for idx_label, label in enumerate(kmeans.labels_):
            if not label in bins:
                bins[label] = [(kmeans_X[idx_label], processed_examples + idx_label)]
            else:
                # dump large bins into partial bin file
                bins[label].append((kmeans_X[idx_label], processed_examples + idx_label))
                if len(bins[label]) > MAX_BIN_SIZE:
                    print('\t\t\tOffloading cluster {} ({})'.format(label, bins_idx.get(label, 0)))
                    crt_bin_file_idx = bins_idx.get(label, 0)
                    crt_bin_file_path = os.path.join(UNCOMPRESSED_BINS_DIR, 'bin-{:04d}-{:04d}.bin'.format(label, crt_bin_file_idx))
                    f = open(crt_bin_file_path, 'wb')
                    pickle.dump(bins[label], f)
                    #print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Dumped another {} examples (total: {})'.format(len(bins[label]), num_dumped_examples))
                    num_dumped_examples += len(bins[label])
                    f.close()
                    bins_idx[label] = crt_bin_file_idx + 1
                    del bins[label]
        
        # dump remaining bins (which do not exceed MAX_BIN_SIZE examples)
        print('Dumping remaining {} bins'.format(len(bins)))
        for idx_label, examples in bins.items():
            print('\t\t\tOffloading remaining items from cluster {} ({})'.format(idx_label, bins_idx.get(idx_label, 0)))
            crt_bin_file_idx = bins_idx.get(idx_label, 0)
            #########################################crt_bin_file_path = os.path.join(UNCOMPRESSED_BINS_DIR, 'bin-{:04d}-{:04d}.bin'.format(label, crt_bin_file_idx)) # <- bug overwriting previous bin files (causing missing indices in the bin files numbering)
            crt_bin_file_path = os.path.join(UNCOMPRESSED_BINS_DIR, 'bin-{:04d}-{:04d}.bin'.format(idx_label, crt_bin_file_idx))
            f = open(crt_bin_file_path, 'wb')
            pickle.dump(bins[idx_label], f)
            #print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Dumped another {} examples (total: {})'.format(len(bins[idx_label]), num_dumped_examples))
            num_dumped_examples += len(bins[idx_label])
            f.close()
            bins_idx[idx_label] = crt_bin_file_idx + 1
        
        # reset the bins contents before moving to the next text record
        bins = dict()
        processed_examples += kmeans_X.shape[0]

        # remove remaining embedding files
        tmp_embds_files = os.listdir(TEMPORARY_EMBEDDINGS_DIR)
        tmp_embds_files.sort()
        for idx_tmp_emb, tmp_emb in enumerate(tmp_embds_files):
            full_tmp_emb_path = os.path.join(TEMPORARY_EMBEDDINGS_DIR, tmp_emb)
            os.remove(full_tmp_emb_path)
        crt_tmp_emb_idx = 0
        initialized_x = False

    print('Remaining bin size: {} | processed kmeans: {} | num loaded: {} | num embedded: {} | kmeans x: {} | num dumped examples: {}'.format(len(bins), processed_examples, num_loaded_examples, num_embedded_examples, kmeans_x_examples, num_dumped_examples))

    # dump final fit version of the kmeans obj, after processing all text records
    crt_kmeans_obj_path = os.path.join(FITTED_MBKS_DIR, 'final-fitted-mbk.bin')
    g = open(crt_kmeans_obj_path, 'wb')
    pickle.dump(kmeans, g)
    g.close()

def assemble_full_emb_X():
    TMP_EMBEDDINGS_DIR = os.path.join(DATASET_DIR, 'tmp-embeddings/')
    UNCOMPRESSED_BINS_DIR = os.path.join(DATASET_DIR, 'bins-uncompressed/')

    bins = dict()
    bins_idx = dict()

    all_embds = os.listdir(TMP_EMBEDDINGS_DIR)
    all_embds.sort()
    X = None
    initialized_X = False
    processed_examples = 0

    kmeans_obj = MiniBatchKMeans(n_clusters = 1000, random_state=0, max_iter=3000000, batch_size=2048)#.partial_fit(X)
    # kmeans fitting options:
    # 1) continuously fit on partial data -> cluster centers might move, examples initially assigned to a cluster might no longer belong to it after further fitting
    # 2) fit on a very small set of data -> all other examples are only transformed (not fitting the kmeans object)
    for idx_emb, emb in enumerate(all_embds):
        #if idx == 33:
        #    break
        print('Concatenating embedding object {}/{}'.format(idx_emb, len(all_embds)))
        data = pickle.load(open(os.path.join(TMP_EMBEDDINGS_DIR, emb), 'rb'))
        if not initialized_X:
            initialized_X = True
            X = data
        else:
            X = np.vstack((X, data))

        if idx_emb > 0 and idx_emb % 10 == 0:
            #kmeans_obj = MiniBatchKMeans(n_clusters = 1000, random_state=0, max_iter=3000000, batch_size=2048).partial_fit(X)
            kmeans_obj = kmeans_obj.partial_fit(X)
            initialized_X = False

            for idx, label in enumerate(kmeans_obj.labels_):
                if not label in bins:
                    bins[label] = [(X[idx], processed_examples + idx)]
                else:
                    bins[label].append((X[idx], processed_examples + idx))
                    if len(bins[label]) > 2000:
                        print('Offloading cluster {} ({})'.format(label, bins_idx.get(label, 0)))
                        crt_bin_file_idx = bins_idx.get(label, 0)
                        f = open(os.path.join(UNCOMPRESSED_BINS_DIR, 'bin-{:04d}-{:04d}.bin'.format(label, crt_bin_file_idx)), 'wb')
                        pickle.dump(bins[label], f)
                        f.close()
                        bins_idx[label] = crt_bin_file_idx + 1
                        del bins[label]
            processed_examples += X.shape[0]
    
    # perform partial fitting with remaining X
    #kmeans_obj = MiniBatchKMeans(n_clusters = 1000, random_state=0, max_iter=3000000, batch_size=2048).partial_fit(X)
    kmeans_obj = kmeans_obj.partial_fit(X)
    for idx, label in enumerate(kmeans_obj.labels_):
        if not label in bins:
            bins[label] = [(X[idx], processed_examples + idx)]
        else:
            bins[label].append((X[idx], processed_examples + idx))
            if len(bins[label]) > 2000:
                print('Offloading cluster {} ({})'.format(label, bins_idx.get(label, 0)))
                crt_bin_file_idx = bins_idx.get(label, 0)
                f = open(os.path.join(UNCOMPRESSED_BINS_DIR, 'bin-{:04d}-{:04d}.bin'.format(label, crt_bin_file_idx)), 'wb')
                pickle.dump(bins[label], f)
                f.close()
                bins_idx[label] = crt_bin_file_idx + 1
                del bins[label]
    
    # dump remaning bin labels which contain < 2000 examples
    for bin_label, bin_examples in bins.items():
        print('Offloading remaining cluster {} ({})'.format(bin_label, bins_idx.get(bin_label, 0)))
        crt_bin_file_idx = bins_idx.get(bin_label, 0)
        f = open(os.path.join(UNCOMPRESSED_BINS_DIR, 'bin-{:04d}-{:04d}.bin'.format(bin_label, crt_bin_file_idx)), 'wb')
        pickle.dump(bins[bin_label], f)
        f.close()
        bins_idx[bin_label] = crt_bin_file_idx + 1
        #del bins[bin_label]

    processed_examples += X.shape[0]

    return kmeans_obj

def store_fitted_kmeans():
    #kmeans = MiniBatchKMeans(n_clusters = 1000, random_state=0, max_iter=3000000, batch_size=2048)
    kmeans = assemble_full_emb_X()
    with open('fitted-mbk-google-colab.bin', 'wb') as g:
        pickle.dump(kmeans, g)

def restore_kmeans():
    kmeans = None
    
    f = open('fitted-mbk-google-colab.bin', 'rb')
    kmeans = pickle.load(f)
    
    return kmeans

def restore_kmeans_path(ckpt_idx):
    kmeans = None
    
    kmeans_path = os.path.join(FITTED_MBKS_DIR, 'fitted-mbk-{:04d}.bin'.format(ckpt_idx))
    f = open(kmeans_path, 'rb')
    kmeans = pickle.load(f)
    
    return kmeans

def standard_construct_bins():
    bins = dict()

    print(len(kmeans.labels_))
    for idx, label in enumerate(kmeans.labels_):
        if idx % 1000 == 0:
            print('Idx: {} | Label: {}'.format(idx, label))
            
    if use_downsampled_embeddings:
        for idx, label in enumerate(kmeans.labels_):
            if not label in bins:
                bins[label] = [(transformed_X[idx], idx)]
                
            else:
                bins[label].append((transformed_X[idx], idx))
    else:
        for idx, label in enumerate(kmeans.labels_):
            if not label in bins:
                bins[label] = [(X[idx], idx)]
                
            else:
                bins[label].append((X[idx], idx))

    cluster_centers = kmeans.cluster_centers_
    
    return bins, cluster_centers

def compressed_construct_bins():
    bins = dict()
    num_examples = 0
    
    for bin_ in range(NUM_CLUSTERS):
        crt_path = os.path.join(DATASET_DIR, 'bins-uncompressed/bin-{:04d}*'.format(bin_))
        print('Looking for files matching \'{}\''.format(crt_path))
        crt_bin_files = glob.glob(crt_path)
        crt_bin_files.sort()
        print('crt_bin_files: {}'.format(crt_bin_files))
        print('-----')
        #bins[bin_] = []
        crt_bin = []
        for bin_file in crt_bin_files:
            print('Opening file --- {}'.format(bin_file))
            f = open(bin_file, 'rb')
            try:
                crt_bin_data = pickle.load(f)
            except OSError as oserror:
                print('Error was {}; attempting again in 5 seconds'.format(oserror))
                time.sleep(5)
                crt_bin_data = pickle.load(f)
            f.close()
            num_examples += len(crt_bin_data)
            #bins[bin_] += crt_bin_data
            crt_bin += crt_bin_data
        
        print('Writing bin {}'.format(bin_))
        with open(os.path.join(DATASET_DIR, 'bins-compressed/bin-{:04d}.bin'.format(bin_)), 'wb') as g:
            pickle.dump(crt_bin, g)
        
    print('Loaded {} examples'.format(num_examples))
    
    #for bin_label, bin_examples in bins.items():
    #    print('Writing bin {}'.format(bin_label))
    #    with open(os.path.join(DATASET_DIR, 'bins-compressed/bin-{:04d}.bin'.format(bin_label)), 'wb') as g:
    #        pickle.dump(bin_examples, g)
    
    return bins

def restore_compressed_bins():
    bins = dict()
    num_examples = 0
    
    compressed_bins = os.listdir(os.path.join(DATASET_DIR, 'bins-compressed'))
    compressed_bins.sort()
    
    for idx_bin, compressed_bin_path in enumerate(compressed_bins):
        if idx_bin % 10 == 0:
            print('Loading bin {}'.format(idx_bin))
        full_compressed_bin_path = os.path.join(DATASET_DIR, 'bins-compressed', compressed_bin_path)
        f = open(full_compressed_bin_path, 'rb')
        crt_bin_data = pickle.load(f)
        f.close()
        
        bins[idx_bin] = crt_bin_data
    
    return bins

if USE_DOWNSAMPLED_EMBEDDINGS:
    transformer = IncrementalPCA(n_components=64, batch_size=512)
    #transformer = transformer.partial_fit(X)
    transformed_X = transformer.fit_transform(X)

def get_closest_cluster_batch(cluster_centers, v):
    cosine_similarities = cosine_similarity(cluster_centers, v.reshape(1, -1))

    return np.argmax(cosine_similarities)

def top_1_closest_examples_batch(bins, cluster_idx, v):
    embeddings_only = np.array(list(zip(*bins[cluster_idx]))[0])
    cosine_similarities = cosine_similarity(embeddings_only, v.reshape(1, -1))

    max_idx = np.argmax(cosine_similarities)

    return bins[cluster_idx][max_idx][1], np.max(cosine_similarities)

def get_closest_k_cluster_batch(cluster_centers, v, k):
    cosine_similarities = cosine_similarity(cluster_centers, v.reshape(1, -1)).squeeze()
    ret = (np.argpartition(cosine_similarities, -k)[-k:])
    ret = ret[np.argsort(cosine_similarities[ret])[::-1]].tolist()
    return ret

def top_k_closest_examples_batch(bins, cluster_idx, v, k):
    zipped_list = list(zip(*bins[cluster_idx]))
    embeddings_only = np.array(zipped_list[0])
    cosine_similarities = cosine_similarity(embeddings_only, v.reshape(1, -1)).squeeze()

    #print('type(cosine_similarities): {}'.format(type(cosine_similarities)))
    #print('shape(cosine_similarities): {}'.format(cosine_similarities.shape))

    max_idx = (np.argpartition(cosine_similarities, -k)[-k:])
    max_idx = max_idx[np.argsort(cosine_similarities[max_idx])[::-1]].tolist()

    #print('Type(max_idx) = {}'.format(type(max_idx)))
    #print('len(max_idx): {}'.format(len(max_idx)))

    return [bins[cluster_idx][i][1] for i in max_idx], cosine_similarities[max_idx]

def retrieve_similar_sentences(inference_sentence, embedder, kmeans_obj, bins=None, ipca_transformer=None, cluster_beam_size=5, sentence_beam_size=5):
    crt_embedding = embedder.batch_encode([inference_sentence]).squeeze()
    crt_embedding = crt_embedding.detach().cpu().numpy()
    if USE_DOWNSAMPLED_EMBEDDINGS:
        crt_embedding = transformer.transform([crt_embedding])[0]

    closest_cluster_idx = get_closest_k_cluster_batch(kmeans_obj.cluster_centers_, crt_embedding, cluster_beam_size)
    closest_sentence_idx_ = []
    
    if bins is None:
        bins = dict()
        for closest_cluster_idx_ in closest_cluster_idx:
            print('Loading cluster {}'.format(closest_cluster_idx_))
            f = open(os.path.join(DATASET_DIR, 'bins-compressed', 'bin-{:04d}.bin'.format(closest_cluster_idx_)), 'rb')
            examples = pickle.load(f)
            f.close()
            bins[closest_cluster_idx_] = examples
    
    for closest_cluster_idx_ in closest_cluster_idx:
        print('-----> Attempting to access elements of cluster {}'.format(closest_cluster_idx_))
        zipped_list = list(zip(*bins[closest_cluster_idx_]))
        if len(zipped_list) == 0:
            continue
        closest_sentence_idx, max_similarity = top_k_closest_examples_batch(bins, closest_cluster_idx_, crt_embedding, sentence_beam_size)
        for closest_sentence_idx__, max_similarity_ in zip(closest_sentence_idx, max_similarity):
            print('Closest vector to sencentes \'{}\' in cluster {} has index {} and similarity {}'.format(inference_sentence, closest_cluster_idx_, closest_sentence_idx__, max_similarity_))
            closest_sentence_idx_.append((closest_sentence_idx__, max_similarity_))
    return closest_sentence_idx_

def get_text_data_full():
    all_text_records = os.listdir(os.path.join(DATASET_DIR, 'text-records/'))
    all_text_records.sort()
    
    sentence_idx_boundaries = dict()
    data_texts = dict()
    running_sentence_num = 0
    for idx_text_record, text_record in enumerate(all_text_records):
        full_text_record_path = os.path.join(DATASET_DIR, 'text-records/', text_record)
        f_text = open(full_text_record_path, 'rb')
        data_text = pickle.load(f_text)
        data_texts[idx_text_record] = data_text
        f_text.close()
        
        flattened_data = list(chain.from_iterable(data_text))
        sentence_idx_boundaries[idx_text_record] = (running_sentence_num, running_sentence_num + len(flattened_data) - 1)
        running_sentence_num += len(flattened_data)
    
    for id_, limits in sentence_idx_boundaries.items():
        print('Text record {:04d} contains sentences between indices (inclusive):\t{}'.format(id_, limits))
   
    return data_texts, sentence_idx_boundaries
    
def get_text_data():
    f_text = open(os.path.join(DATASET_DIR, 'text-records/', '0000-record.rec'), 'rb')
    data_text = pickle.load(f_text)
    #print(type(data_text))
    #print(len(data_text))
    
    return data_text

def train(embedder):
    create_tmp_embeddings(embedder)
    #store_fitted_kmeans()
    #kmeans = restore_kmeans()
    #bins = compressed_construct_bins()

def inference(test_sentence, embedder, kmeans, bins=None, ipca_transformer=None, cluster_beam_size=5, sentence_beam_size=5):
    data_text = get_text_data()
    start = time.time()
    target_idx = retrieve_similar_sentences(test_sentence, embedder, kmeans, bins=bins, ipca_transformer=ipca_transformer, cluster_beam_size=cluster_beam_size, sentence_beam_size=sentence_beam_size)
    end = time.time()
    print('Sentence retrieval took {} seconds\n'.format(end - start))

    
    sentences_to_return = []
    for target_idx_, max_similarity_ in target_idx:
        idx_ = 0
        crt_scenario = 0

        while idx_ < target_idx_:
            if idx_ + len(data_text[crt_scenario]) > target_idx_:
                break
            idx_ += len(data_text[crt_scenario])
            crt_scenario += 1

        #print('Idx_ = {} | crt_scenario = {}\n'.format(idx_, crt_scenario))

        num_neighbours_to_display = 3
        for i in range(-num_neighbours_to_display,
                       num_neighbours_to_display + 1):
            if target_idx_ - idx_ + i >= 0 and target_idx_ - idx_ + i < len(data_text[crt_scenario]):
                if i == 0:
                    print('\t{} -> {} (similarity {})'.format(target_idx_ + i, data_text[crt_scenario][target_idx_ - idx_ + i], max_similarity_))
                    sentences_to_return.append((data_text[crt_scenario][target_idx_ - idx_ + i], max_similarity_))
                #else:
                #    print('{} -> {}'.format(target_idx_ + i, data_text[crt_scenario][target_idx_ - idx_ + i]))
        #print('-----')
    #for idx, example in enumerate(data_text[crt_scenario]):
    #    print('{} -> {}'.format(idx_ + idx, example))
    
    
    sentences_to_return.sort(key=itemgetter(1), reverse=True)
    
    return sentences_to_return

def inference_multiple_text_records(test_sentence, embedder, kmeans, bins=None, ipca_transformer=None, cluster_beam_size=5, sentence_beam_size=5):
    data_text_, sentence_idx_boundaries = get_text_data_full()
    start = time.time()
    target_idx = retrieve_similar_sentences(test_sentence, embedder, kmeans, bins=bins, ipca_transformer=ipca_transformer, cluster_beam_size=cluster_beam_size, sentence_beam_size=sentence_beam_size)
    end = time.time()
    print('Sentence retrieval took {} seconds\n'.format(end - start))

    
    sentences_to_return = []
    for target_idx_, max_similarity_ in target_idx:
        idx_ = 0
        crt_scenario = 0
        backup_target_idx_ = target_idx_
        
        for id_, limits in sentence_idx_boundaries.items():
            if target_idx_ >= limits[0] and target_idx_ <= limits[1]:
                data_text = data_text_[id_]
                target_idx_ -= limits[0]
                break

        while idx_ < target_idx_:
            if idx_ + len(data_text[crt_scenario]) > target_idx_:
                break
            idx_ += len(data_text[crt_scenario])
            crt_scenario += 1

        #print('Idx_ = {} | crt_scenario = {}\n'.format(idx_, crt_scenario))

        num_neighbours_to_display = 3
        for i in range(-num_neighbours_to_display,
                       num_neighbours_to_display + 1):
            if target_idx_ - idx_ + i >= 0 and target_idx_ - idx_ + i < len(data_text[crt_scenario]):
                if i == 0:
                    print('\t{} (original ID: {}) -> {} (similarity {})'.format(target_idx_ + i, backup_target_idx_, data_text[crt_scenario][target_idx_ - idx_ + i], max_similarity_))
                    sentences_to_return.append((data_text[crt_scenario][target_idx_ - idx_ + i], max_similarity_))
                #else:
                #    print('{} -> {}'.format(target_idx_ + i, data_text[crt_scenario][target_idx_ - idx_ + i]))
        #print('-----')
    #for idx, example in enumerate(data_text[crt_scenario]):
    #    print('{} -> {}'.format(idx_ + idx, example))
    
    
    sentences_to_return.sort(key=itemgetter(1), reverse=True)
    
    return sentences_to_return

def inference_selected_bins(test_sentence, embedder, kmeans, ipca_transformer, cluster_beam_size=5, sentence_beam_size=5):
    data_text = get_text_data()
    start = time.time()
    target_idx = retrieve_similar_sentences(test_sentence, embedder, kmeans, bins, ipca_transformer, cluster_beam_size, sentence_beam_size)
    end = time.time()
    print('Sentence retrieval took {} seconds\n'.format(end - start))

    
    sentences_to_return = []
    for target_idx_, max_similarity_ in target_idx:
        idx_ = 0
        crt_scenario = 0

        while idx_ < target_idx_:
            if idx_ + len(data_text[crt_scenario]) > target_idx_:
                break
            idx_ += len(data_text[crt_scenario])
            crt_scenario += 1

        #print('Idx_ = {} | crt_scenario = {}\n'.format(idx_, crt_scenario))

        num_neighbours_to_display = 3
        for i in range(-num_neighbours_to_display,
                       num_neighbours_to_display + 1):
            if target_idx_ - idx_ + i >= 0 and target_idx_ - idx_ + i < len(data_text[crt_scenario]):
                if i == 0:
                    print('\t{} -> {} (similarity {})'.format(target_idx_ + i, data_text[crt_scenario][target_idx_ - idx_ + i], max_similarity_))
                    sentences_to_return.append((data_text[crt_scenario][target_idx_ - idx_ + i], max_similarity_))
                #else:
                #    print('{} -> {}'.format(target_idx_ + i, data_text[crt_scenario][target_idx_ - idx_ + i]))
        #print('-----')
    #for idx, example in enumerate(data_text[crt_scenario]):
    #    print('{} -> {}'.format(idx_ + idx, example))
    
    
    sentences_to_return.sort(key=itemgetter(1), reverse=True)
    
    return sentences_to_return

def plot_bins_distribution(bins):
    sns.set_context(rc = {'patch.linewidth': 0.0})
    for k, v in bins.items():
        print('Bin {} | length {}'.format(k, len(v)))

    ordered_bins = OrderedDict(sorted(bins.items()))
    ordered_keys = list(ordered_bins.keys())
    ordered_bin_sizes = list(map(lambda x : len(x), list(ordered_bins.values())))
    
    #plt.bar(ordered_keys, ordered_bin_sizes, color='blue', label='Number of examples per bin')
    sns.barplot(x=ordered_keys, y=ordered_bin_sizes, color='blue', label='Number of examples per bin')
    plt.xticks(np.arange(0, len(ordered_bins) + 1, 100))
    plt.legend(loc='best')
    plt.xlabel('Bin number')
    plt.ylabel('Number of examples')
    plt.title('Distribution of cluster sizes')
    
    plt.show()

def cluster_nlu_instances(kmeans_obj, bins):
    chosen_sentences_per_intent = dict()

    with open(NLU_YAML_PATH, 'r') as f:
        try:
            intent_data = yaml.safe_load(f)
        except yaml.YAMLError as exc:
            print(exc)
            
    nlu = intent_data['nlu']
    for idx_intent, nlu_entry in enumerate(nlu):
        print('Checking intent {}/{}'.format(idx_intent + 1, len(nlu)))
    
        intent = nlu_entry['intent']
        chosen_sentences_per_intent[intent] = set()
        examples = nlu_entry['examples']
        split_examples = examples.split('\n')[:-1] # ignore last training newline producing and empty string
        split_examples = list(map(lambda x : x[2:], split_examples)) # remove leading hypen and whitespace separator
        
        for idx_example, split_example in enumerate(split_examples):
            print('\tRetrieving similar instances for example {}/{}'.format(idx_example + 1, len(split_examples)))
            similar_sentences = inference(split_example, EMBEDDER, kmeans_obj, bins, None, cluster_beam_size=CLUSTER_BEAM_SIZE, sentence_beam_size=SENTENCE_BEAM_SIZE)
            
            for idx_retrieved, similar_sentence in enumerate(similar_sentences):
                #print('Similar sentence: {}'.format(similar_sentence))
                req_obj = {'text':similar_sentence[0]}
                response = requests.post(REQUEST_URL, data=json.dumps(req_obj)).json()
                response_intent_name = response['intent']['name']
                response_intent_confidence = response['intent']['confidence']
                
                #print('Response is {}'.format(response))
                #print('Response intent name: {} | response_intent_confidence: {}'.format(response_intent_name, response_intent_confidence))
                #print('-----')
                if response_intent_name == intent and response_intent_confidence > INTENT_CONFIDENCE_THRESHOLD:
                    if not similar_sentence[0].lower() in split_examples:
                        chosen_sentences_per_intent[intent].add(similar_sentence[0].lower()) # only add unique lowercase sentences in order to avoid inserting duplicate retrieved examples
                
    for intent, retrieved_sentences in chosen_sentences_per_intent.items():
        print('Intent: {} - retrieved {} additional instances:'.format(intent, len(retrieved_sentences)))
        for retrieved_sentence in retrieved_sentences:
            print('\t- {}'.format(retrieved_sentence))
        print('-----------------------------------')

def main():
    #train(EMBEDDER)
    
    #store_fitted_kmeans()
    
    
    #kmeans = restore_kmeans()
    #bins = compressed_construct_bins()
    
    ################kmeans = restore_kmeans()  # <---- needs to be done
    ################bins = restore_compressed_bins() # <---- needs to be done
    #plot_bins_distribution(bins)
    
    #similar_sentences = inference('could you tell me more about your projects?', EMBEDDER, kmeans, None, bins, cluster_beam_size=CLUSTER_BEAM_SIZE, sentence_beam_size=SENTENCE_BEAM_SIZE)
    #similar_sentences = inference('why do you want to work with us?', EMBEDDER, kmeans, None, bins, cluster_beam_size=CLUSTER_BEAM_SIZE, sentence_beam_size=SENTENCE_BEAM_SIZE)
    #similar_sentences = inference('glad to meet you!', EMBEDDER, kmeans, None, bins, cluster_beam_size=CLUSTER_BEAM_SIZE, sentence_beam_size=SENTENCE_BEAM_SIZE)
    #similar_sentences = inference('what are your responsabilities at the current workplace?', EMBEDDER, kmeans, None, cluster_beam_size=CLUSTER_BEAM_SIZE, sentence_beam_size=SENTENCE_BEAM_SIZE)
    #similar_sentences = inference('How much time do you need?', EMBEDDER, kmeans, None, cluster_beam_size=CLUSTER_BEAM_SIZE, sentence_beam_size=SENTENCE_BEAM_SIZE)
    #similar_sentences = inference('What do you like to do in your free time?', EMBEDDER, kmeans, None, cluster_beam_size=CLUSTER_BEAM_SIZE, sentence_beam_size=SENTENCE_BEAM_SIZE)
    #similar_sentences = inference('What is your experience in the field of 3d printing?', EMBEDDER, kmeans, None, cluster_beam_size=CLUSTER_BEAM_SIZE, sentence_beam_size=SENTENCE_BEAM_SIZE)
    
    #for sentence, similarity in similar_sentences:
    #    print('\t{}\t|\tsimilarity: {}'.format(sentence, similarity))
    
    '''
    start_retrieval_time = time.time()
    cluster_nlu_instances(kmeans, bins)
    end_retrieval_time = time.time()
    print('Retrieved sentences in {} seconds'.format(end_retrieval_time - start_retrieval_time))
    '''
    
    #create_tmp_embeddings(EMBEDDER)
    #kmeans = restore_kmeans_path(14)
    #bins = compressed_construct_bins()
    
    ##f = open('fitted-mbk-google-colab.bin', 'rb')
    ##kmeans = pickle.load(f)
    ##f.close()
    ##bins = restore_compressed_bins()
    ##similar_sentences = inference('why do you want to work with us?', EMBEDDER, kmeans, bins=bins, ipca_transformer=None, cluster_beam_size=CLUSTER_BEAM_SIZE, sentence_beam_size=SENTENCE_BEAM_SIZE)

    
    f = open(os.path.join(FITTED_MBKS_DIR, 'final-fitted-mbk.bin'), 'rb')
    kmeans = pickle.load(f)
    f.close()
    #bins = restore_compressed_bins()
    bins = None
    '''
    similar_sentences = inference_multiple_text_records('well i guess i could start the second day', EMBEDDER, kmeans, bins=bins, ipca_transformer=None, cluster_beam_size=CLUSTER_BEAM_SIZE, sentence_beam_size=SENTENCE_BEAM_SIZE)
    similar_sentences = inference_multiple_text_records('i am happy', EMBEDDER, kmeans, bins=bins, ipca_transformer=None, cluster_beam_size=CLUSTER_BEAM_SIZE, sentence_beam_size=SENTENCE_BEAM_SIZE)
    similar_sentences = inference_multiple_text_records('what are your responsabilities at the current workplace?', EMBEDDER, kmeans, bins=bins, ipca_transformer=None, cluster_beam_size=CLUSTER_BEAM_SIZE, sentence_beam_size=SENTENCE_BEAM_SIZE)
    similar_sentences = inference_multiple_text_records('what is your experience in the field of 3d printing?', EMBEDDER, kmeans, bins=bins, ipca_transformer=None, cluster_beam_size=CLUSTER_BEAM_SIZE, sentence_beam_size=SENTENCE_BEAM_SIZE)
    similar_sentences = inference_multiple_text_records('glad to meet you!', EMBEDDER, kmeans, bins=bins, ipca_transformer=None, cluster_beam_size=CLUSTER_BEAM_SIZE, sentence_beam_size=SENTENCE_BEAM_SIZE)
    similar_sentences = inference_multiple_text_records('could you tell me more about your projects?', EMBEDDER, kmeans, bins=bins, ipca_transformer=None, cluster_beam_size=CLUSTER_BEAM_SIZE, sentence_beam_size=SENTENCE_BEAM_SIZE)
    similar_sentences = inference_multiple_text_records('no, but i will finish the university soon.', EMBEDDER, kmeans, bins=bins, ipca_transformer=None, cluster_beam_size=CLUSTER_BEAM_SIZE, sentence_beam_size=SENTENCE_BEAM_SIZE)
    '''
    start_main = time.time()
    similar_sentences = inference_multiple_text_records('what is your experience in the field of 3d printing?', EMBEDDER, kmeans, bins=bins, ipca_transformer=None, cluster_beam_size=4, sentence_beam_size=4)
    end_main = time.time()
    print('Retrieved {} examples in {} seconds'.format(CLUSTER_BEAM_SIZE * SENTENCE_BEAM_SIZE, end_main - start_main))
    

if __name__ == '__main__':
    main()